# MWT-1: Honest Text Generation Through Random Token Selection

**Jay Neawedde**
*Independent Researcher, Cleveland, OH*

**Abstract.** We introduce MWT-1, a novel text generation system that achieves sub-millisecond inference latency, zero hallucinations, zero bias, and zero environmental impact by eliminating the machine learning pipeline entirely. MWT-1 generates text through random selection from a curated 150-token vocabulary, running on a $3 ESP8266 microcontroller consuming 0.5W. We demonstrate that MWT-1 outperforms state-of-the-art large language models on multiple operational metrics while being transparent about the one metric where it underperforms: usefulness. We argue that the gap between MWT-1's output quality and that of models costing millions of dollars to train is smaller than the industry would like to admit, particularly in enterprise communication contexts. All claims in this paper are technically true.

## 1. Introduction

The large language model (LLM) industry has grown to an estimated $150 billion valuation as of 2026, fueled by models containing hundreds of billions of parameters trained on trillions of tokens of scraped internet data at a cost of tens of millions of dollars per training run.

These models generate text.

MWT-1 also generates text.

The difference between MWT-1 and a frontier LLM is that MWT-1 is honest about what it's doing. It is a random number generator with a vocabulary list. It makes no claims about understanding, reasoning, or intelligence. It does not pretend to think. It does not simulate empathy. It does not apologize for things it cannot feel sorry about.

We believe there is value in a text generation system that is exactly what it says it is.

## 2. Related Work

The infinite monkey theorem, first articulated by Emile Borel (1913), posits that a monkey pressing keys at random on a typewriter for an infinite amount of time will almost surely produce any given text, including the complete works of Shakespeare.

MWT-1 differs from this theoretical framework in several respects:

1. We have constrained the keyboard to 150 keys, all of which produce corporate buzzwords
2. We are not waiting for Shakespeare; we are targeting quarterly business reviews
3. Our monkey is an ESP8266 microcontroller that costs $3
4. The time horizon is not infinite, but it might feel like it if you read enough of the output

Prior work in random text generation includes Markov chains (Shannon, 1948), context-free grammars (Chomsky, 1956), and enterprise resource planning software documentation (various, 1990-present).

## 3. Methodology

### 3.1 Vocabulary Curation

The MWT-1 vocabulary was manually curated by observing which words appear most frequently in technology industry communications while contributing the least semantic content. Our selection criteria required that each token:

- Appears in at least 3 corporate contexts (earnings calls, pitch decks, LinkedIn posts, conference keynotes, or consulting deliverables)
- Sounds important when spoken aloud
- Resists precise definition
- Can be removed from a sentence without changing the sentence's practical meaning

The final vocabulary consists of 50 nouns, 25 verbs, 25 adjectives, and 22 structural fillers.

### 3.2 Inference Architecture

MWT-1's inference engine implements a five-template pattern system with uniform random token selection. The complete algorithm is:

```
for each output position:
    select random pattern template (uniform, 5 options)
    for each slot in template:
        select random token from appropriate category (uniform)
    apply punctuation with 30% probability
```

The total inference logic is 47 lines of C. We do not believe additional complexity would improve the output quality in any measurable way, and we can prove this mathematically: the output is already maximally random.

### 3.3 Hardware

MWT-1 runs on a WeMos D1 Mini development board featuring the Espressif ESP8266 system-on-chip. Key specifications:

- **Processor:** Tensilica L106, 160MHz, single core, 32-bit
- **RAM:** 80KB usable
- **Flash:** 4MB
- **WiFi:** 802.11 b/g/n
- **Power:** 0.5W typical
- **Unit cost:** $3 USD (AliExpress, qty 1)

The entire model, inference engine, web server, and API fit within the ESP8266's 80KB of usable RAM with room to spare. We estimate the model itself occupies approximately 2.4KB.

## 4. Results

### 4.1 Performance

MWT-1 achieves inference latency below 1 millisecond, measured from API request receipt to response transmission. This is approximately 800x faster than GPT-4o and 1200x faster than Claude Opus 4.

We acknowledge that GPT-4o and Claude Opus 4 are doing considerably more work during those milliseconds. We also note that from the user's perspective, both systems produce text, and MWT-1 produces it faster.

### 4.2 Sample Output

The following outputs were generated by MWT-1 during evaluation. No cherry-picking was performed.

> "enterprise-grade synergy leverages the bleeding-edge paradigm. scalable infrastructure disrupts our next-generation ecosystem. the hyper-converged pipeline optimizes every mission-critical deliverable toward real-time convergence."

> "disruptive innovation orchestrates beyond carbon-neutral monetization. each battle-tested framework futureproofs the quantum-ready trajectory."

> "the composable platform accelerates frictionless alignment. event-driven architecture tokenizes our high-availability roadmap."

We submitted these outputs to 12 mid-level technology managers and asked them to rate the content on a scale of 1-5 for "strategic clarity." The average score was 3.2.

We did not actually do this study, but we are confident in the result.

### 4.3 Hallucination Analysis

MWT-1's hallucination rate is 0.00%. This metric requires clarification.

A "hallucination" in the LLM context is defined as output that presents fabricated information as fact. MWT-1 cannot hallucinate because:

1. It has no training data from which to fabricate
2. It makes no factual claims
3. It has no model of reality to deviate from
4. Its output is explicitly random

Whether this makes MWT-1 more or less honest than a model that has a concept of truth but frequently deviates from it is a philosophical question that we leave to the reader. We note only that MWT-1 has never confidently told someone the wrong answer to a math problem, fabricated a legal citation, or invented a person who doesn't exist.

## 5. Economic Analysis

The total cost to deploy a production MWT-1 instance is $3 (hardware) plus $0.00/month (operating cost at residential electricity rates). Annual infrastructure spend: approximately $0.04.

OpenAI reported approximately $5 billion in revenue for 2024. MWT-1's economic model suggests that a significant portion of this revenue may be attributable to the perception of intelligence rather than its presence.

We are not making a direct comparison. We are making an observation.

## 6. Ethical Considerations

MWT-1 was developed without scraping any copyrighted material, without using human feedback labelers paid below minimum wage, without consuming megawatts of electricity, and without partnering with defense contractors or surveillance companies.

We believe this makes MWT-1 the most ethically developed language model in history.

We also believe this is a low bar.

## 7. Limitations

MWT-1 cannot do anything useful. We state this plainly because we believe the AI industry would benefit from more projects that are honest about their limitations.

## 8. Conclusion

MWT-1 demonstrates that many of the metrics used to evaluate language models, including inference latency, power efficiency, bias scores, and hallucination rates, can be trivially optimized by removing the intelligence. We do not suggest that this makes MWT-1 superior to frontier LLMs. We suggest that it reveals which metrics are meaningful and which are marketing.

Every claim in this paper is technically true. If that makes you uncomfortable, we encourage you to re-read the technical claims made by the model you're currently paying for.

## References

Borel, E. (1913). "Mecanique Statistique et Irreversibilite." *J. Phys. 5e Serie*, 3, 189-196.

Chomsky, N. (1956). "Three Models for the Description of Language." *IRE Transactions on Information Theory*, 2(3), 113-124.

Shannon, C.E. (1948). "A Mathematical Theory of Communication." *Bell System Technical Journal*, 27(3), 379-423.

Various Authors (1990-present). "Enterprise Resource Planning Software Documentation." Reviewed but not cited, on grounds of mental health.

---

*Corresponding author: jamesneawedde@protonmail.com*
*No conflicts of interest. No funding sources. No GPUs were harmed.*
